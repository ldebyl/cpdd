CPDD - Copy with Deduplication
==============================

A high-performance cross-platform UNIX utility for intelligent file copying with
content-based deduplication. When copying files, cpdd detects duplicates by
comparing against a reference directory and creates hard or symbolic links
instead of duplicate copies, dramatically reducing disk usage. It can do this
even when duplicates iunder different directories or with different filenames.

DESIGN PHILOSOPHY
-----------------
Copy utilities such as 'cp' duplicate all files, wasting disk space when identical
content already exists on the destination volume. cpdd solves this by:

- Content-aware copying: Detects identical files regardless of name/location
- Performance optimization: Only calculates expensive checksums when necessary
- Space efficiency: Uses filesystem links instead of duplicate storage
- UNIX philosophy: Does one thing well with clean command-line interface
- Cross-platform: Works on all POSIX-compliant systems

The tool is designed for scenarios where you're reorganising large file collections,
creating backups, or consolidating data where duplicates are likely but scattered
across different directory structures with different naming conventions.

A common use case could be for large photo, music or video collections, where snapshots
of these collections in time exist ocross multiple volumes, but with radically
different organisational structures. Consolidating multiple backups to one
volume would typically result in signficant duplication.

Whilst utilities such as fdupes exist to identify duplicates across filesystem
branches, it cannot recreate a branch with links. Other utilities that can
create hard and soft links require that the content be on the same volume, which
necessitates that all content (including duplicates) can physically fit onto a
single volume - which often isn't the case.

FEATURES
--------
- Support for multiple reference directories
- Can create hard or soft links to duplicate files
- Efficient and robust matching algorithm
- Comphrensive test framework
- Minimal C implementation with minimal depedancies, tested across macOS, Linux & BSD

IMPLEMENTATION
--------------
cpdd uses a three-stage matching algorithm:

1. **Size Filtering** (fastest): Files with different sizes cannot be identical
2. **MD5 Checksums** (fast): Cryptographic hash comparison catches most duplicates
3. **Byte-by-byte** (thorough): Final verification ensures 100% accuracy

**Performance Optimizations:**
- Sorted array duplicate detection: O(n log n) size duplicate identification using binary search
- Two-pass reference scanning: Only calculates MD5 for reference files with duplicate sizes
- Early size filtering: Skips expensive operations for obviously different files
- Streaming comparison: No unnecessary memory allocation for large files
- Filesystem linking: Zero-copy deduplication via hard/soft links

**Reference Directory Scanning:**
When a reference directory is specified, cpdd first scans it to build an index:
1. Collect all reference file paths and sizes (fast, metadata only)
2. Build sorted arrays of seen/duplicate sizes for O(log n) lookups
3. Where there are two or more reference files with identical sizes, they are
   marked as being eligible for MD5 hash calculation
4. Each source file is processed.
   - If there are no files in the reference directories with the same size,
     the file is copied.
   - If there is exactly one file in the reference directories with the same size,
     a byte-wiese comparison is performed, and the file is either copied
     or linked.
   - If there are two or more files with the same size in the reference
     directories, lazily evaluate and cache the MD5 sum for each reference file
     and compare hashes before a full byte-wise comparison.

**Copy Operation:**
For each source file, cpdd:
- Checks size against reference index (instant filter)
- Calculates source MD5 only if reference matches exist
- Performs byte-level verification before linking
- Falls back to normal copy if no matches found

BUILDING
--------
Requirements:
- GCC or compatible C compiler
- POSIX-compliant system (Linux, macOS, FreeBSD, OpenBSD, NetBSD)
- Make utility

Build everything:
    make                    # Builds cpdd and testgen

Build individually:
    make cpdd               # Main program only
    make syndir             # Test data generator only

Development:
    make debug              # Build with debug symbols
    make test               # Run comprehensive test suite
    make clean              # Remove build artifacts

Installation:
    make install            # Install to /usr/local/bin
    make uninstall          # Remove from system

USAGE
-----
cpdd [OPTIONS] SOURCE DESTINATION

Core Options:
    -r, --reference DIR   Reference directory (enables deduplication, defaults to hard links)
    -H, --hard-link       Create hard links to reference files (default with -r)
    -s, --symbolic-link   Create symbolic links to reference files
    -R, --recursive       Copy directories recursively
    -v, --verbose         Show detailed operation progress (multiple levels: -v, -vv, -vvv)
    -n, --no-clobber      Never overwrite existing files
    -i, --interactive     Prompt before overwriting files
    --preserve=LIST       Preserve specified attributes (mode,ownership,timestamps,all)
    --stats               Show final copy/link statistics
    -h, --help            Display help message

USE CASES & EXAMPLES
-------------------

**1. Media Library Consolidation**
You have photos scattered across multiple directories with different names:
    cpdd -r ~/Photos/masters -R ~/Downloads/photos ~/Photos/all
Result: Duplicates are hard-linked to existing files, saving gigabytes

**2. Backup Deduplication**
Creating space-efficient backups of large datasets:
    cpdd -r /backup/previous -R /home/data /backup/current
Result: Only changed files consume additional space

**3. Code Repository Cleanup**
Consolidating multiple project copies with shared dependencies:
    cpdd -r ~/projects/shared -R ~/Downloads/project-copy ~/projects/clean
Result: Shared libraries and assets are linked, not duplicated

**4. Document Archive Organization**
Merging document collections while preserving originals:
    cpdd -r ~/archive -s -R ~/Downloads/docs ~/archive/new
Result: Symbolic links preserve original locations while avoiding duplicates

**5. Large File Processing**
Working with video files, datasets, or other large content:
    cpdd -r /storage/originals -v -R /processing/input /processing/output
Result: Verbose output shows space savings, large files aren't re-read unnecessarily

PERFORMANCE CHARACTERISTICS
---------------------------
**Best Case**: Reference directory with many large, uniquely-sized files
- Most files skip MD5 calculation entirely
- Near-instant size-based filtering
- Minimal disk I/O beyond normal copy operations

**Typical Case**: Mixed file sizes with ~30% duplication rate
- MD5 calculated for ~50% of reference files (those with size duplicates)
- Source files calculated on-demand during matching
- Significant space savings through linking

**Worst Case**: All files have identical sizes but different content
- All files require MD5 calculation and byte-level comparison
- Still faster than naive approaches due to early MD5 filtering
- Performance similar to traditional copy with verification

TEST DATA GENERATION
--------------------
cpdd includes 'syndir' for creating realistic test scenarios with configurable 
file distributions and duplication patterns:

    syndir [OPTIONS] REF_ROOT SRC_ROOT

Core Options:
    -f, --files COUNT     Number of files to generate (default: 100)
    -d, --dirs COUNT      Number of directories to create (default: 10)
    -p, --percent PCT     Percentage of source files that duplicate reference (0-100, default: 30)
    -v, --verbose         Show generation progress
    -h, --help            Display help message

Advanced File Size Control:
    --size-p50 SIZE       50th percentile file size in bytes (default: 4096)
    --size-p95 SIZE       95th percentile file size in bytes (default: 65536)
    --size-max SIZE       Maximum file size in bytes (default: 1048576)
    --size-scale FACTOR   Scale all file sizes by this factor (default: 1.0)

**How syndir Works:**
1. Creates reference directory with random files using realistic size distribution
2. Creates source directory where specified percentage have identical content to reference files
3. Duplicate files are placed in different locations with different names
4. Perfect for testing cpdd's deduplication performance across various scenarios

**Example Workflows:**

Basic usage:
    # Default: 100 files, 10 dirs, 30% duplicates
    ./syndir /tmp/ref /tmp/src
    
    # High duplication scenario
    ./syndir -f 200 -d 20 -p 80 /tmp/ref /tmp/src

Performance testing:
    # Generate large test set with verbose output
    ./syndir -v -f 1000 -d 50 -p 60 /tmp/ref /tmp/src
    
    # Test cpdd performance and measure space savings
    time ./cpdd -r /tmp/ref -v --stats /tmp/src /tmp/dest
    
    # Compare disk usage
    du -sh /tmp/ref /tmp/src /tmp/dest

Custom file size distributions:
    # Small files (documents/configs)
    ./syndir --size-p50 1024 --size-p95 8192 --size-max 32768 /tmp/ref /tmp/src
    
    # Large files (media/datasets) 
    ./syndir --size-scale 1000 -f 50 /tmp/ref /tmp/src

Complete demo workflow:
    # Run the included demo script
    ./examples/demo.sh
    
    # Interactive testing of multiple scenarios  
    ./examples/usage.sh

TESTING
-------
Comprehensive test suite covering:
- Basic file operations and edge cases
- Recursive directory traversal
- Hard and symbolic link creation
- Content matching algorithm correctness
- Command-line argument validation
- Cross-platform compatibility

Run tests:
    make test               # Full test suite
    ./examples/demo.sh      # Interactive demonstration

TECHNICAL DETAILS
-----------------
**File Matching**: Three-tier approach balances speed and accuracy
**Memory Usage**: Minimal - processes files in streaming fashion
**Disk I/O**: Optimized to avoid unnecessary reads of large files
**Error Handling**: Graceful fallback to normal copy on link failures
**Platform Support**: Pure POSIX C99 with OpenSSL for hashing

**Limitations**:
- Hard links require same filesystem (soft links work cross-filesystem)
- MD5 collisions theoretically possible (mitigated by byte-level verification)
- Reference directory must be readable during entire operation
- Memory usage scales with unique file sizes in reference directory (typically minimal)

**Known Issues**:
- Realloc failure handling could be more robust in edge cases
- Terminal detection for progress display needs refinement on some systems

LICENSE
-------
This software is designed as a practical tool for system administrators,
developers, and power users managing large file collections. Use responsibly
and in accordance with your local laws and organizational policies.

Built with modern C practices for reliability, performance, and portability.
